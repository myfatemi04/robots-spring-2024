{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1092ebe1-3b50-424b-b460-a27679925893",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "\n",
    "I create a standardized `.jsonl` format. Each line of the `.jsonl` file is a dictionary with the following information:\n",
    " - `image`: A (potentially relative) path to the target image. Relative paths are used when standard benchmarks are used, and absolute paths vary between users.\n",
    " - `width`, `height`: The dimensions of the image.\n",
    " - `references`: A list of the format:\n",
    "    - `caption`: A caption describing the object.\n",
    "    - `xyxy`: A bounding box, in xyxy format, in pixels.\n",
    " - `known_absent_captions`: A list of object text descriptions that are known to not be in the image.\n",
    "\n",
    "Each of the detection APIs will follow a standardized format.\n",
    "Input:\n",
    " - `image`: An image\n",
    " - `captions`: A batch of captions to test for\n",
    "Output: A list of object detections, each with:\n",
    "   - `xyxy`: A bounding box, in xyxy format, in pixels.\n",
    "   - `logits`: A list of binary logits (representing log\\[P(true)/P(false)\\]).\n",
    "   - `scores`: A list of normalized (0...1) scores for each of the captions. These may not sum to 1 if none of the captions match, or if multiple captions are not mutually exclusive in some way. They are calculated directly by applying the sigmoid function to the `logits` output.\n",
    "   - `caption`: An optional string which includes the caption generated for that chosen object.\n",
    "\n",
    "Providing logit and score outputs for each detection allows us to calculate average precision for false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6428aed5-2c86-44df-bb7e-23b7ff3b5a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detect_cas import owlv2, get_caption_logit, generate_caption_phi3\n",
    "import numpy as np\n",
    "\n",
    "# This wrapper makes it easy to generate captions.\n",
    "class PaliGemmaWrapper:\n",
    "    def __init__(self, base_model_id, finetune_checkpoint=None, device=\"cuda\"):\n",
    "        token = os.environ['HUGGINGFACE_ACCESS_TOKEN']\n",
    "        model_id = \"google/paligemma-3b-mix-224\"\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            finetune_checkpoint,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            token=token,\n",
    "            device_map=device,\n",
    "        )\n",
    "        processor = PaliGemmaProcessor.from_pretrained(model_id, token=token)\n",
    "        \n",
    "        self.model = model\n",
    "        self.procesor = processor\n",
    "        \n",
    "    def generate_caption(self, image, bbox):\n",
    "        # Tokenize the bounding box.\n",
    "        (x1, y1, x2, y2) = bbox\n",
    "        # Scale to integer coordinated in a 1024x1024 image.\n",
    "        x1_quantized = int((x1 / image.width) * 1024)\n",
    "        y1_quantized = int((y1 / image.height) * 1024)\n",
    "        x2_quantized = int((x2 / image.width) * 1024)\n",
    "        y2_quantized = int((y2 / image.height) * 1024)\n",
    "        bbox_tokenized = f\"<loc{x1_quantized:04d}><loc{y1_quantized:04d}><loc{x2_quantized:04d}><loc{y2_quantized:04d}>\"\n",
    "        \n",
    "        prefix = f\"Describe {bbox_tokenized}\"\n",
    "        \n",
    "        inputs = processor(prefix, image.convert(\"RGB\"), return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_new_tokens=32)\n",
    "            \n",
    "        return output\n",
    "\n",
    "# Uses OwlV2's raw text embedding similarity, and chooses the object detection with the highest similarity score.\n",
    "class OwlV2Selector:\n",
    "    def __init__(self, owlv2):\n",
    "        self.owlv2 = owlv2\n",
    "        \n",
    "    def detect(self, image, captions):\n",
    "        return self.owlv2(image, captions)\n",
    "\n",
    "# Uses object detections from OwlV2, and physical attributes from phi-3-vision, to select the target object.\n",
    "class CropAndCaptionSelector:\n",
    "    def __init__(self, owlv2):\n",
    "        self.owlv2 = owlv2\n",
    "        \n",
    "    def detect(self, image, targets):\n",
    "        preliminary_detections = self.owlv2(image, targets)\n",
    "        generated_captions = [generate_caption_phi3(image, detection['xyxy']) for detection in preliminary_detections]\n",
    "        detections = []\n",
    "        for i in range(len(preliminary_detections)):\n",
    "            logits = np.array([get_caption_logit(generated_captions[i], target) for target in targets])\n",
    "            detection = {\n",
    "                \"xyxy\": preliminary_detections[i][\"xyxy\"],\n",
    "                \"logits\": logits,\n",
    "                \"scores\": np.sigmoid(logits),\n",
    "                \"caption\": generated_captions[i],\n",
    "            }\n",
    "            \n",
    "        return detections\n",
    "\n",
    "\"\"\"\n",
    "Uses object detections from OwlV2, and a finetuned Pali 3 model to generate captions for the corresponding object detections.\n",
    "\n",
    "In the longer term... a nice paper would be to do this (the object detection, and reasoning), end-to-end. For example:\n",
    "\n",
    "```\n",
    "<image>\n",
    "\n",
    "<user> Locate the person in the white shirt. </user>\n",
    "\n",
    "<agent>\n",
    "\n",
    "Let's first locate all the people in the image.\n",
    "<bbox 1>: A person with a white shirt.\n",
    "<bbox 2>: ...\n",
    "\n",
    "The most likely match for the criteria is <bbox 1>.\n",
    "\n",
    "</agent>\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "class ChainOfThoughtSelector:\n",
    "    \"\"\"\n",
    "    Very similar to the crop-and-caption selector, just uses a different captioning backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, owlv2, vlm: PaliGemmaWrapper):\n",
    "        self.owlv2 = owlv2\n",
    "        self.vlm = vlm\n",
    "        \n",
    "    def detect(self, image, targets):\n",
    "        preliminary_detections = self.owlv2(image, targets)\n",
    "        generated_captions = [self.vlm.generate_caption(image, detection['xyxy']) for detection in preliminary_detections]\n",
    "        detections = []\n",
    "        for i in range(len(preliminary_detections)):\n",
    "            logits = np.array([get_caption_logit(generated_captions[i], target) for target in targets])\n",
    "            detection = {\n",
    "                \"xyxy\": preliminary_detections[i][\"xyxy\"],\n",
    "                \"logits\": logits,\n",
    "                \"scores\": np.sigmoid(logits),\n",
    "                \"caption\": generated_captions[i],\n",
    "            }\n",
    "            \n",
    "        return detections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764087b-546a-4497-ad1b-f11864f514a9",
   "metadata": {},
   "source": [
    "## Processing `.jsonl` datasets\n",
    "\n",
    "Now, let's create a pipeline through which we can process validation datasets and generate nice metrics (e.g, AP, AP50, AP75). I don't think AP, AP50, and AP75 should deviate very much, because OwlV2 is already pretty good; I would be more concerned about false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ea35a-d903-4fb7-82f7-1f8527a025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cas_env",
   "language": "python",
   "name": "cas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
