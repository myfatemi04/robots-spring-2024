import base64
import io
import re

import matplotlib.pyplot as plt
import numpy as np
from openai import OpenAI
from PIL import Image
from generate_object_candidates import detect, draw_set_of_marks

load_llava = False
if load_llava:
    from transformers import AutoProcessor, LlavaForConditionalGeneration

    llava_model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-13b-hf").to('cuda')
    llava_processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-13b-hf")
else:
    llava_model = None
    llava_processor = None

def llava(image, text, max_new_tokens=384):
    assert load_llava, "`load_llava` was not set to True, so the llava models are not loaded."

    prompt = f"<image>{text}"
    inputs = llava_processor(text=prompt, images=image, return_tensors="pt").to('cuda')
    
    generate_ids = llava_model.generate(**inputs, max_new_tokens=max_new_tokens)
    generated = llava_processor.batch_decode(
        generate_ids[:, inputs.input_ids.shape[1]:],
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]
    
    return generated

def pil_image_to_base64(image):
    """
    Convert a PIL Image to base64 format.

    Args:
    - image (PIL Image): Input image.

    Returns:
    - str: Base64 representation of the image.
    """
    # Convert the PIL Image to bytes
    img_byte_array = io.BytesIO()
    image.save(img_byte_array, format='PNG')
    img_byte_array = img_byte_array.getvalue()

    # Convert the bytes to base64
    base64_image = base64.b64encode(img_byte_array).decode('utf-8')
    return base64_image

def scale_image(image, max_dim=1024):
    """
    Resize the image so that the maximum dimension is `max_dim`.
    
    Args:
    - image (PIL Image): Input image
    - max_dim (int): Maximum dimension (width or height) of the output image
    
    Returns:
    - PIL Image: Resized image
    """
    width, height = image.size
    if max(width, height) > max_dim:
        if width > height:
            new_width = max_dim
            new_height = int(max_dim * height / width)
        else:
            new_height = max_dim
            new_width = int(max_dim * width / height)
        image = image.resize((new_width, new_height), Image.BICUBIC)
    return image

def gpt4v(image, text, max_new_tokens=384):
    client = OpenAI()
    
    base64_image = pil_image_to_base64(image)

    response = client.chat.completions.create(
      model="gpt-4-vision-preview",
      messages=[
        {
          "role": "user",
          "content": [
            {"type": "text", "text": text},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            },
          ],
        }
      ],
      max_tokens=max_new_tokens,
    )
    
    return response.choices[0].message.content

def parse_plan(plan):
    # Extract the first bulleted list.
    lines = plan.strip().split("\n")
    steps = []
    for line in lines:
        # $ = end of line
        pattern = re.compile(fr"(\d+)\. Object: (.+?), Action: (.+?)$")
        match = pattern.match(line)
        if match is None:
            break
            
        # counter = int(match.group(1))
        object_ = match.group(2)
        action = match.group(3)
        
        steps.append((action, object_))
        
    return steps

def block(title, s):
    print((">" * 10), title)
    print(s)
    print(("<" * 10), title)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def generate_action_candidates(image, task, vlm='gpt4v'):
    plt.title("Input Image.")
    plt.imshow(image)
    plt.axis('off')
    plt.show()
    
    # Simple prompt engineering.
    if vlm == 'llava':
        prompt = f"""
USER: {task}. How could I start doing this? Give me a very concrete / literal set of steps.
First, write "Observations:" followed by some of your observations. Then, write "Plan:", followed by a very specific set of objects
and actions to manipulate them.
ASSISTANT: Observations:"""
        
        reasoning = "Observations: " + vlm(image, prompt)
    
        block("Reasoning Generated By LLaVA", reasoning)
        
        prompt_2 = f"""{prompt}{reasoning}
        USER: Now, give instructions to a robot to follow these steps. Write your answers in the format "Object: [object], Action: [action]".
        ASSISTANT: 1. Object:"""
        plan_str = ("1. Object: " if vlm == llava else '') + vlm(image, prompt_2).strip()
        
        block("Parsed Plan Section", plan_str)
    else:
        prompt = f"""{task}. Given this image, how could I start doing this?e
Give me a very concrete / literal set of steps, as if you were instructing a robot
with very little semantic understanding, who can only use visual or physical descriptions
of how to interact with objects.

Begin your response with "Observations:" followed by some of your observations. Use this time to understand
the layout of the scene and what you could do to complete the task at hand.

Then, write "Plan:", followed by very concrete, literal action descriptions, of the format "Object: [object], Action: [action]".
Begin your answer with the string "1. ", and continue with a numbered list of the actions to take. Notes:
 - "Object" should be a text description of what object the robot should interact with in the scene. It should not refer to the robot arm itself.
 - "Action" should refer to the action the robot arm should take with the object.
 - Both "Object" and "Action" should be very brief descriptions (i.e. 3-5 words).
 - Ensure you adhere to the format "Object: [object], Action: [action]". The two text descriptions must be on the same line, separated by a comma.
""".strip()
        
        # response = gpt4v(image, prompt)
        response = """
Observations: In the image, there is a stove with a pan on top of one of the burners. To the right of the stove, there is a clear bottle that appears to contain oil. To the left of the stove, there is a robotic arm, which is presumably what needs to be instructed to carry out the task. There are various other items on the countertop, but they are not relevant for pouring oil into the pan.

Plan:
1. Object: Clear oil bottle, Action: Grasp bottle firmly
2. Object: Clear oil bottle, Action: Lift bottle from countertop
3. Object: Clear oil bottle, Action: Position bottle over the center of the pan
4. Object: Clear oil bottle, Action: Tilt bottle to pour oil
5. Object: Clear oil bottle, Action: Return bottle to upright position to stop pouring
6. Object: Clear oil bottle, Action: Place bottle back on countertop
7. Object: Robot arm, Action: Move away from stove area
        """.strip()
        
        block("Raw GPT-4V Response", response)
        
        plan_start_index = response.index("Plan:")
        observations = response[:plan_start_index]
        
        block("Parsed Observations Section", response)
        
        plan_str = response[plan_start_index + 5:].strip()
        
        block("Parsed Plan Section", plan_str)
    
    plan = parse_plan(plan_str)
    
    if len(plan) == 0:
        print("ERROR: Plan parsing failed.")
        return
    
    block("Parsed Action Literals", '\n'.join(str(x) for x in plan))
    
    # First step.
    action, object_ = plan[0]
    
    image = image.convert("RGB")
    detection = detect(image, object_)
    
    # Overlay this with the image.
    set_of_marks = draw_set_of_marks(image, detection)

    plt.imshow(set_of_marks)
    plt.show()

    # plt.title(f"Task: {task}. First Step: '{action.lower()}' the below object.")
    # plt.imshow(image)
    # plt.imshow(detection, alpha=sigmoid(detection))
    # plt.axis('off')
    # plt.show()

if __name__ == '__main__':
    if False:
        parse_plan("""
    1. Object: Laptop, Action: Close
    2. Object: Laptop, Action: Put in backpack
    3. Object: Backpack, Action: Remove from floor
    4. Object: Backpack, Action: Place on chair
    5. Object: Laptop, Action: Open
    6. Object: Laptop, Action: Put in backpack
    7. Object: Backpack, Action: Remove from chair
    8. Object: Backpack, Action: Place on floor
    9. Object: Laptop, Action: Close
    10. Object: Laptop, Action: Put in backpack
    """)

    image = scale_image(Image.open("droid_sample.png").convert("RGB"))
    generate_action_candidates(image, "Pour some oil in the pan", vlm='gpt4v')
