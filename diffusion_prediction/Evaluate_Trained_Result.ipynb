{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac617dc-02c7-4d17-aa69-6d28a1bdd81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757f5009-1c30-452f-973e-6f3ef9bfd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/gsk6me/huggingface_cache'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e311cd-5977-4a2d-9851-aadf07e9a582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gsk6me/miniconda3/envs/py310/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/gsk6me/miniconda3/envs/py310/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-03-03 16:47:02.531047: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import torch.utils.data\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.utils import ContextManagers\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKLTemporalDecoder, DDPMScheduler, StableVideoDiffusionPipeline, UNetSpatioTemporalConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel, compute_snr\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.utils import check_min_version, deprecate, is_wandb_available, make_image_grid\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "\n",
    "from rt1_dataset_wrapper import RT1Dataset\n",
    "\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40020328-2a66-4cda-b708-85da02a91a56",
   "metadata": {},
   "source": [
    "## Load Pipelines\n",
    "\n",
    "We have a base pipeline, which is only image-conditioned, and we have a LoRA pipeline, which has had low-rank weight updates made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283870f0-8308-4a81-917b-c0d076598f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipeline_lora\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1801fa-dbcd-4087-935f-a0cf6bc8ede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000fbbed755d41ec99960244e40cc87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the base pipeline\n",
    "# pipeline_base = StableVideoDiffusionPipeline.from_pretrained(\n",
    "#     \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "# pipeline_base = pipeline_base.to(device='cuda')\n",
    "pipename = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
    "pipeline_lora = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    pipename,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipeline_lora = pipeline_lora.to(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdee42-5c83-4c8e-8ac6-df781cc80552",
   "metadata": {},
   "source": [
    "## Inject LoRA\n",
    "\n",
    "Because the regular pipeline doesn't have an easy way to load LoRA adapters directly, I just manually add them to the U-Net weights. We need to use HuggingFace's SafeTensors library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea86763-11d1-4645-85f8-bf1ef40905c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "rank = 256\n",
    "unet_lora_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=rank,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    ")\n",
    "pipeline_lora.unet.add_adapter(unet_lora_config)\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "# CHECKPOINT = 'snrgamma5-intentional-overfit-100-epochs'\n",
    "CHECKPOINT = '004_edm_preconditioning'\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(f\"./experiments/004_edm_preconditioning_overfit_checkpoints/checkpoint-500/unet/diffusion_pytorch_model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "# with safe_open(f\"./sd-model-finetuned-checkpoints/{CHECKPOINT}/unet/diffusion_pytorch_model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)\n",
    "        # print(\"Loading tensor\", k)\n",
    "\n",
    "pipeline_lora.unet.load_state_dict(tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe504d-7620-4937-84d4-4f61e9016b4e",
   "metadata": {},
   "source": [
    "## Unify with Text Encoder\n",
    "\n",
    "Here, we create a unified trajectory synthesis pipeline, which is conditioned on text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d5d167-fc3c-4acb-a4c4-305b1abe4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_and_image_conditioned_video_diffusion_model import VisualTrajectorySynthesizer\n",
    "\n",
    "# Load the text encoder. This only works for the LoRA model.\n",
    "text_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "text_encoder = CLIPTextModel.from_pretrained(text_encoder_path).to(device='cuda', dtype=torch.float16)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(text_encoder_path)\n",
    "\n",
    "pipeline_lora_unified = VisualTrajectorySynthesizer.from_stable_video_diffusion_pipeline(pipeline_lora, text_encoder)\n",
    "del pipeline_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0efbc-8f94-42ae-a0f2-ca5b1716aaff",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We only trained on the first $20$ samples from the RT-1 dataset, and of those, we only trained on the first $25$ frames of each video. Let's see how they turned out. We will use the custom class I wrote, `RT1Dataset`, for this. We also need to do a bit of image processing before we can directly send inputs into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07c44e6-7bbf-4647-b163-3ac9a0284e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rt1_dataset_wrapper import RT1Dataset\n",
    "\n",
    "# Originally from `args`\n",
    "image_height = 256\n",
    "image_width = 320\n",
    "vae_image_processor = pipeline_lora_unified.image_processor\n",
    "\n",
    "# We don't technically even have to do this because the `vae_image_processor` is already called in the StableVideoDiffusionPipeline\n",
    "def collate_fn(batch):\n",
    "    # input: (text, image_sequence)[]\n",
    "    # return: (text batch, text attention masks, text sequence lengths, images)\n",
    "    text_batch = [text for (text, imgseq) in batch]\n",
    "    tokenization = tokenizer(text_batch, padding='longest', return_tensors='pt')\n",
    "    text_tokens = tokenization['input_ids']\n",
    "    text_attention_masks = tokenization['attention_mask']\n",
    "\n",
    "    imgseqs = [vae_image_processor.preprocess(imgseq, height=image_height, width=image_width) for (_, imgseq) in batch]\n",
    "\n",
    "    return (text_tokens, text_attention_masks, torch.stack(imgseqs))\n",
    "\n",
    "dataset = RT1Dataset('/scratch/gsk6me/WORLDMODELS/datasets/rt-1-data-release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e1885d-4e68-426b-a111-515588e940d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.2546, -0.3799,  0.0656,  ...,  0.1837,  0.0816, -0.3835],\n",
      "         [ 0.3843,  0.6553, -0.3020,  ...,  1.1660,  0.7383, -0.9102],\n",
      "         [ 0.2107,  0.1082, -2.0918,  ..., -0.1875,  1.8955, -1.3477],\n",
      "         [ 0.1605,  0.4241,  0.0748,  ..., -0.1385, -0.3008,  0.8154],\n",
      "         [ 0.3262,  0.2437, -0.3157,  ...,  0.2820, -1.0430, -0.0968]]],\n",
      "       device='cuda:0', dtype=torch.float16), pooler_output=tensor([[ 0.3262,  0.2437, -0.3157,  ...,  0.2820, -1.0430, -0.0968]],\n",
      "       device='cuda:0', dtype=torch.float16), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 1024])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa521e49f7543e39b3023e52106b673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 11.3.0 (conda-forge gcc 11.3.0-19)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1674566204550/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1674566204550/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1674566204550/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1674566204550/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1674566204550/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-pthreads --enable-vaapi --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1674566204550/_build_env/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'generated_videos/checkpoints/004_edm_preconditioning/generated_lora_0_raw.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:03.57, start: 0.000000, bitrate: 960 kb/s\n",
      "  Stream #0:0[0x1](und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 320x256 [SAR 1:1 DAR 5:4], 958 kb/s, 7 fps, 7 tbr, 14336 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "File 'generated_videos/checkpoints/004_edm_preconditioning/generated_lora_0.mp4' already exists. Overwrite? [y/N] Not overwriting - exiting\n"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "import os\n",
    "\n",
    "video_export_dir = f'generated_videos/checkpoints/{CHECKPOINT}'\n",
    "if not os.path.exists(video_export_dir):\n",
    "    os.makedirs(video_export_dir)\n",
    "\n",
    "for n in range(1):\n",
    "    text, image_sequence = dataset[n]\n",
    "    input_ids = tokenizer(text, return_tensors='pt').input_ids\n",
    "    \n",
    "    # Use the first image as conditioning\n",
    "    # Note that because training only occurred over the first 25 frames of each video,\n",
    "    # the model has not seen robots performing task completion yet.\n",
    "    import PIL.Image\n",
    "    \n",
    "    conditioning_image_tensor = image_sequence[0]\n",
    "    conditioning_image_np = np.array((image_sequence[0] * 255).permute(1, 2, 0)).astype(np.uint8)\n",
    "    conditioning_image_pil = PIL.Image.fromarray(conditioning_image_np)\n",
    "\n",
    "    frames = pipeline_lora_unified.custom_call(\n",
    "        conditioning_image_pil,\n",
    "        input_ids,\n",
    "        width=image_width,\n",
    "        height=image_height,\n",
    "        was_trained_with_edm_preconditioning=True,\n",
    "    )\n",
    "    export_to_video(frames[0], f\"{video_export_dir}/generated_lora_{n}_raw.mp4\", fps=7)\n",
    "\n",
    "    # frames = pipeline_base(conditioning_image_pil, width=image_width, height=image_height)\n",
    "    # export_to_video(frames[0][0], f\"generated_non_lora_{n}_raw.mp4\", fps=7)\n",
    "\n",
    "    os.system(f\"ffmpeg -i {video_export_dir}/generated_lora_{n}_raw.mp4 {video_export_dir}/generated_lora_{n}.mp4\")\n",
    "    os.system(f\"rm {video_export_dir}/generated_lora_{n}_raw.mp4\")\n",
    "    \n",
    "    # os.system(f\"ffmpeg -i generated_non_lora_{n}_raw.mp4 generated_non_lora_{n}.mp4\")\n",
    "    # os.system(f\"rm generated_non_lora_{n}_raw.mp4\")\n",
    "\n",
    "# conditioning_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "023177c0-88fd-4fd6-b2a5-9da2b8eb16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKLTemporalDecoder\n",
    "vae = AutoencoderKLTemporalDecoder.from_pretrained( # type: ignore\n",
    "    pipename, subfolder=\"vae\"\n",
    ")\n",
    "vae = vae.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d78ebffc-03ff-4859-b0a9-3ee298a41e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, images = dataset[0]\n",
    "preprocessed_images = pipeline_lora_unified.image_processor.preprocess(images, width=320, height=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f16e3cd8-59ff-44d1-919a-b4a639b48116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_video(tensor_sequence, index):\n",
    "    # Convert the PyTorch tensors to numpy arrays\n",
    "    frames = [(tensor.permute(1, 2, 0).cpu().numpy()[:, :, ::-1] * 255).astype(np.uint8) for tensor in tensor_sequence]\n",
    "\n",
    "    # Define the video writer\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('_generated.mp4', fourcc, 30.0, (width, height))\n",
    "\n",
    "    # Write each frame to the video\n",
    "    for frame in frames:\n",
    "        out.write(frame.astype(np.uint8))\n",
    "\n",
    "    # Release the video writer\n",
    "    out.release()\n",
    "\n",
    "    with open(os.devnull, 'wb') as devnull:\n",
    "        subprocess.check_call(['ffmpeg', '-y', '-i', '_generated.mp4', f'generated_{index}.mp4'], stdout=devnull, stderr=devnull)\n",
    "\n",
    "    os.system('rm _generated.mp4')\n",
    "    \n",
    "    return Video(f'generated_{index}.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0dd88437-979f-479a-9bc4-6a11979797a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"generated_original_18.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"generated_autoencoder_18.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"generated_original_180.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"generated_autoencoder_180.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"generated_original_1800.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"generated_autoencoder_1800.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"generated_original_18000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"generated_autoencoder_18000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = pipeline_lora_unified.vae\n",
    "dtype = torch.float16\n",
    "\n",
    "for index in [18, 180, 1800, 18000]:\n",
    "    text_description, video = dataset[index]\n",
    "    \n",
    "    print(\"Original Video\")\n",
    "    display(render_video(video, 'original_' + str(index)))\n",
    "    \n",
    "    # Now, let's see what it looks like if we pass it in and out of the VAE.\n",
    "    with torch.no_grad():\n",
    "        latents = []\n",
    "        for image in video:\n",
    "            encoded = vae.encode(\n",
    "                pipeline_lora_unified.image_processor.preprocess(\n",
    "                    image.to(device='cuda', dtype=dtype),\n",
    "                    height=256, width=320,\n",
    "                )\n",
    "            )\n",
    "            latents.append(\n",
    "                encoded.latent_dist.sample()[0] # * vae.config['scaling_factor']\n",
    "            )\n",
    "        latents = torch.stack(latents)\n",
    "        \n",
    "        # Decode latents in chunks of 8\n",
    "    \n",
    "        returned = []\n",
    "        i = 0\n",
    "        while i < len(latents):\n",
    "            frames = latents[i:i + 8]\n",
    "            returned.append(vae.decode(frames, num_frames=len(frames)).sample)\n",
    "            i += 8\n",
    "    \n",
    "        returned = (1 + torch.cat(returned)) / 2\n",
    "        returned = torch.clamp(returned, 0, 1)\n",
    "    \n",
    "    print(\"Autoencoder Video\")\n",
    "    display(render_video(returned, 'autoencoder_' + str(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65acb7d5-4287-47c4-bb4b-a5f375d2e044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 256, 320])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd9264b1-f576-47cc-956b-fc2a339c7572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 open bottom drawer\n",
      "1 pick rxbar blueberry\n",
      "2 move water bottle near blue plastic bottle\n",
      "3 pick brown chip bag from top drawer and place on counter\n",
      "4 pick banana from white bowl\n",
      "5 open bottom drawer\n",
      "6 close bottom drawer\n",
      "7 move rxbar blueberry near blue chip bag\n",
      "8 place coke can into middle drawer\n",
      "9 open middle drawer\n",
      "10 open bottom drawer\n",
      "11 pick coke can from bottom drawer and place on counter\n",
      "12 move water bottle near 7up can\n",
      "13 pick 7up can from top drawer and place on counter\n",
      "14 pick 7up can\n",
      "15 close middle drawer\n",
      "16 move rxbar chocolate near apple\n",
      "17 place blue plastic bottle into bottom drawer\n",
      "18 open top drawer\n",
      "19 close bottom drawer\n"
     ]
    }
   ],
   "source": [
    "for n in range(20):\n",
    "    print(n, dataset[n][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355c780-665a-4e01-bc33-95fe02e7bb80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
